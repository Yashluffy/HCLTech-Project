{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c20cd0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "%pip install langchain langchain-community langchain-openai pymupdf faiss-cpu pydantic python-dotenv \n",
    "%pip install langchain-ollama\n",
    "%pip install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b14875d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "%pip install sentence-transformers langchain-huggingface \n",
    "!pip install ipywidgets\n",
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "078a7268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell -2 Imports and API Setup\n",
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# NOTICE: No OpenAI imports here anymore!\n",
    "from langchain_ollama import ChatOllama # New free LLM\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "# # Securely enter your API Key if not already set in environment\n",
    "# if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "#     os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OpenAI API Key: \")\n",
    "\n",
    "# Configuration\n",
    "PDF_FILES = [\n",
    "    \"sample-service-manual.pdf\",  # The Car Manual\n",
    "    \"HAF-F16.pdf\",                # The Jet Manual\n",
    "    \"motercycles.pdf\"\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a3fb438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking Fleet Status...\n",
      "   ‚úÖ READY: CAR -> sample-service-manual.pdf\n",
      "   ‚úÖ READY: JET -> HAF-F16.pdf\n",
      "   ‚úÖ READY: BIKE -> motercycles.pdf\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 3: Configuration & Fleet Setup ---\n",
    "import os\n",
    "\n",
    "# 1. Define your \"Fleet\"\n",
    "# Map the vehicle type to the specific PDF filename you uploaded\n",
    "FLEET_CONFIG = {\n",
    "    \"car\": \"sample-service-manual.pdf\",       # Ford/Car Manual\n",
    "    \"jet\": \"HAF-F16.pdf\",                     # F-16 Jet Manual\n",
    "    \"bike\": \"motercycles.pdf\"             # Ducati Bike Manual\n",
    "}\n",
    "\n",
    "# 2. Verify files exist\n",
    "print(\"üîç Checking Fleet Status...\")\n",
    "files_ready = True\n",
    "for v_type, filename in FLEET_CONFIG.items():\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"   ‚ö†Ô∏è MISSING: {filename} (Please upload this file!)\")\n",
    "        files_ready = False\n",
    "    else:\n",
    "        print(f\"   ‚úÖ READY: {v_type.upper()} -> {filename}\")\n",
    "\n",
    "if not files_ready:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Some files are missing. The code will skip them.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed1b781b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Fleet Ingestion...\n",
      "\n",
      "üìò Processing CAR Manual: sample-service-manual.pdf...\n",
      "   ‚úÖ Added 1030 chunks from car.\n",
      "\n",
      "üìò Processing JET Manual: HAF-F16.pdf...\n",
      "   ‚úÖ Added 682 chunks from jet.\n",
      "\n",
      "üìò Processing BIKE Manual: motercycles.pdf...\n",
      "   ‚úÖ Added 827 chunks from bike.\n",
      "\n",
      "üéâ Total Fleet Knowledge Base: 2539 chunks ready for embedding.\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 4: Smart \"Fleet\" Extraction Strategy ---\n",
    "import pdfplumber\n",
    "import os\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def process_pdf_with_header_injection(pdf_path, batch_size=5):\n",
    "    \"\"\"\n",
    "    Reads the PDF. Hunts for TRUE header rows (containing 'Nm', 'lb-ft', etc.) \n",
    "    to preserve context in dense tables.\n",
    "    \"\"\"\n",
    "    smart_docs = []\n",
    "    \n",
    "    # Safety check\n",
    "    if not os.path.exists(pdf_path):\n",
    "        return []\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            # 1. Try Table Extraction First\n",
    "            table = page.extract_table({\n",
    "                \"vertical_strategy\": \"text\", \n",
    "                \"horizontal_strategy\": \"text\"\n",
    "            })\n",
    "            \n",
    "            if table:\n",
    "                headers = None\n",
    "                data_start_idx = 0\n",
    "                \n",
    "                # Scan first 5 rows for a \"Unit Header\" (Nm, lb-ft, etc.)\n",
    "                for idx, row in enumerate(table[:5]):\n",
    "                    row_str = \" \".join([str(c).lower() for c in row if c])\n",
    "                    if \"nm\" in row_str or \"lb-ft\" in row_str or \"description\" in row_str or \"symptom\" in row_str:\n",
    "                        headers = row\n",
    "                        data_start_idx = idx + 1\n",
    "                        break\n",
    "                \n",
    "                if headers:\n",
    "                    # Clean headers\n",
    "                    clean_headers = [str(h).replace('\\n', ' ') if h else f\"Col_{j}\" for j, h in enumerate(headers)]\n",
    "                    data_rows = table[data_start_idx:]\n",
    "                    \n",
    "                    current_batch = []\n",
    "                    for row_idx, row in enumerate(data_rows):\n",
    "                        clean_row = [str(cell).replace('\\n', ' ') if cell else \"N/A\" for cell in row]\n",
    "                        \n",
    "                        if len(clean_headers) == len(clean_row):\n",
    "                            # Contextual Row: \"Component: Bolt, Nm: 17...\"\n",
    "                            row_context = \", \".join([f\"{h}: {r}\" for h, r in zip(clean_headers, clean_row)])\n",
    "                            current_batch.append(row_context)\n",
    "                        \n",
    "                        # Chunking: Group 5 rows together\n",
    "                        if len(current_batch) >= batch_size or row_idx == len(data_rows) - 1:\n",
    "                            if current_batch:\n",
    "                                doc = Document(\n",
    "                                    page_content=\"\\n\".join(current_batch),\n",
    "                                    metadata={\"source\": pdf_path, \"page\": i + 1, \"type\": \"table_chunk\"}\n",
    "                                )\n",
    "                                smart_docs.append(doc)\n",
    "                                current_batch = []\n",
    "                else:\n",
    "                    # No recognizable header? Treat as text.\n",
    "                    text = page.extract_text()\n",
    "                    if text:\n",
    "                        smart_docs.append(Document(page_content=text, metadata={\"source\": pdf_path, \"page\": i+1}))\n",
    "            else:\n",
    "                # Fallback: No table found, just raw text\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    smart_docs.append(Document(page_content=text, metadata={\"source\": pdf_path, \"page\": i+1}))\n",
    "\n",
    "    return smart_docs\n",
    "\n",
    "# --- EXECUTE FLEET INGESTION ---\n",
    "all_chunks = []\n",
    "print(\"üöÄ Starting Fleet Ingestion...\")\n",
    "\n",
    "if 'FLEET_CONFIG' in globals():\n",
    "    for vehicle_type, pdf_file in FLEET_CONFIG.items():\n",
    "        if os.path.exists(pdf_file):\n",
    "            print(f\"\\nüìò Processing {vehicle_type.upper()} Manual: {pdf_file}...\")\n",
    "            try:\n",
    "                # Extract\n",
    "                file_chunks = process_pdf_with_header_injection(pdf_file)\n",
    "                \n",
    "                # Tag metadata (Crucial for the AI to know which vehicle it is)\n",
    "                for c in file_chunks:\n",
    "                    c.metadata[\"vehicle_type\"] = vehicle_type\n",
    "                \n",
    "                all_chunks.extend(file_chunks)\n",
    "                print(f\"   ‚úÖ Added {len(file_chunks)} chunks from {vehicle_type}.\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error processing {pdf_file}: {e}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è SKIPPING {vehicle_type}: File '{pdf_file}' not found.\")\n",
    "else:\n",
    "    print(\"‚ùå Error: FLEET_CONFIG not found. Please run Cell 3 first.\")\n",
    "\n",
    "# Final Handover to next cells\n",
    "chunks = all_chunks \n",
    "print(f\"\\nüéâ Total Fleet Knowledge Base: {len(chunks)} chunks ready for embedding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d95aca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∑ Scanning sample-service-manual.pdf for diagrams...\n",
      "   [+] Saved Diagram: p9_img0.png\n",
      "   [+] Saved Diagram: p10_img0.png\n",
      "   [+] Saved Diagram: p17_img0.png\n",
      "   [+] Saved Diagram: p28_img0.png\n",
      "   [+] Saved Diagram: p28_img1.png\n",
      "   [+] Saved Diagram: p33_img2.png\n",
      "   [+] Saved Diagram: p37_img2.png\n",
      "   [+] Saved Diagram: p40_img0.png\n",
      "   [+] Saved Diagram: p44_img0.png\n",
      "   [+] Saved Diagram: p46_img1.png\n",
      "   [+] Saved Diagram: p49_img0.png\n",
      "   [+] Saved Diagram: p49_img1.png\n",
      "   [+] Saved Diagram: p53_img0.png\n",
      "   [+] Saved Diagram: p54_img1.png\n",
      "   [+] Saved Diagram: p56_img1.png\n",
      "   [+] Saved Diagram: p57_img0.png\n",
      "   [+] Saved Diagram: p66_img0.png\n",
      "   [+] Saved Diagram: p66_img1.png\n",
      "   [+] Saved Diagram: p69_img1.png\n",
      "   [+] Saved Diagram: p71_img2.png\n",
      "   [+] Saved Diagram: p74_img0.png\n",
      "   [+] Saved Diagram: p75_img1.png\n",
      "   [+] Saved Diagram: p82_img0.png\n",
      "   [+] Saved Diagram: p87_img0.png\n",
      "   [+] Saved Diagram: p87_img1.png\n",
      "   [+] Saved Diagram: p90_img1.png\n",
      "   [+] Saved Diagram: p92_img2.png\n",
      "   [+] Saved Diagram: p95_img0.png\n",
      "   [+] Saved Diagram: p98_img0.png\n",
      "   [+] Saved Diagram: p99_img1.png\n",
      "   [+] Saved Diagram: p102_img0.png\n",
      "   [+] Saved Diagram: p103_img1.png\n",
      "   [+] Saved Diagram: p105_img0.png\n",
      "   [+] Saved Diagram: p106_img0.png\n",
      "   [+] Saved Diagram: p109_img0.png\n",
      "   [+] Saved Diagram: p109_img1.png\n",
      "   [+] Saved Diagram: p112_img1.png\n",
      "   [+] Saved Diagram: p115_img0.png\n",
      "   [+] Saved Diagram: p116_img0.png\n",
      "   [+] Saved Diagram: p117_img0.png\n",
      "   [+] Saved Diagram: p118_img0.png\n",
      "   [+] Saved Diagram: p119_img0.png\n",
      "   [+] Saved Diagram: p123_img1.png\n",
      "   [+] Saved Diagram: p124_img1.png\n",
      "   [+] Saved Diagram: p132_img0.png\n",
      "   [+] Saved Diagram: p133_img9.png\n",
      "   [+] Saved Diagram: p135_img0.png\n",
      "   [+] Saved Diagram: p136_img9.png\n",
      "   [+] Saved Diagram: p145_img1.png\n",
      "   [+] Saved Diagram: p147_img1.png\n",
      "   [+] Saved Diagram: p154_img0.png\n",
      "   [+] Saved Diagram: p182_img0.png\n",
      "   [+] Saved Diagram: p185_img1.png\n",
      "   [+] Saved Diagram: p194_img0.png\n",
      "   [+] Saved Diagram: p196_img0.png\n",
      "   [+] Saved Diagram: p201_img0.png\n",
      "   [+] Saved Diagram: p202_img0.png\n",
      "   [+] Saved Diagram: p216_img0.png\n",
      "   [+] Saved Diagram: p236_img1.png\n",
      "   [+] Saved Diagram: p248_img0.png\n",
      "   [+] Saved Diagram: p251_img0.png\n",
      "   [+] Saved Diagram: p254_img0.png\n",
      "   [+] Saved Diagram: p255_img0.png\n",
      "   [+] Saved Diagram: p256_img0.png\n",
      "   [+] Saved Diagram: p270_img0.png\n",
      "   [+] Saved Diagram: p279_img3.png\n",
      "   [+] Saved Diagram: p283_img1.png\n",
      "   [+] Saved Diagram: p285_img0.png\n",
      "   [+] Saved Diagram: p292_img0.png\n",
      "   [+] Saved Diagram: p293_img0.png\n",
      "   [+] Saved Diagram: p294_img0.png\n",
      "   [+] Saved Diagram: p294_img1.png\n",
      "   [+] Saved Diagram: p298_img0.png\n",
      "   [+] Saved Diagram: p302_img0.png\n",
      "   [+] Saved Diagram: p306_img0.png\n",
      "   [+] Saved Diagram: p311_img0.png\n",
      "   [+] Saved Diagram: p315_img1.png\n",
      "   [+] Saved Diagram: p321_img0.png\n",
      "   [+] Saved Diagram: p327_img0.png\n",
      "   [+] Saved Diagram: p330_img0.png\n",
      "   [+] Saved Diagram: p336_img0.png\n",
      "   [+] Saved Diagram: p353_img0.png\n",
      "   [+] Saved Diagram: p355_img0.png\n",
      "   [+] Saved Diagram: p357_img0.png\n",
      "   [+] Saved Diagram: p363_img0.png\n",
      "   [+] Saved Diagram: p374_img0.png\n",
      "   [+] Saved Diagram: p376_img0.png\n",
      "   [+] Saved Diagram: p377_img2.png\n",
      "   [+] Saved Diagram: p379_img0.png\n",
      "   [+] Saved Diagram: p386_img0.png\n",
      "   [+] Saved Diagram: p393_img0.png\n",
      "   [+] Saved Diagram: p395_img0.png\n",
      "   [+] Saved Diagram: p396_img0.png\n",
      "   [+] Saved Diagram: p397_img1.png\n",
      "   [+] Saved Diagram: p399_img1.png\n",
      "   [+] Saved Diagram: p400_img0.png\n",
      "   [+] Saved Diagram: p402_img0.png\n",
      "   [+] Saved Diagram: p403_img0.png\n",
      "   [+] Saved Diagram: p404_img0.png\n",
      "   [+] Saved Diagram: p405_img0.png\n",
      "   [+] Saved Diagram: p405_img1.png\n",
      "   [+] Saved Diagram: p410_img0.png\n",
      "   [+] Saved Diagram: p412_img2.png\n",
      "   [+] Saved Diagram: p413_img2.png\n",
      "   [+] Saved Diagram: p415_img1.png\n",
      "   [+] Saved Diagram: p417_img1.png\n",
      "   [+] Saved Diagram: p418_img0.png\n",
      "   [+] Saved Diagram: p422_img0.png\n",
      "   [+] Saved Diagram: p429_img3.png\n",
      "   [+] Saved Diagram: p434_img0.png\n",
      "   [+] Saved Diagram: p437_img0.png\n",
      "   [+] Saved Diagram: p443_img0.png\n",
      "   [+] Saved Diagram: p456_img1.png\n",
      "   [+] Saved Diagram: p464_img0.png\n",
      "   [+] Saved Diagram: p473_img0.png\n",
      "   [+] Saved Diagram: p489_img2.png\n",
      "   [+] Saved Diagram: p490_img0.png\n",
      "   [+] Saved Diagram: p491_img0.png\n",
      "   [+] Saved Diagram: p491_img1.png\n",
      "   [+] Saved Diagram: p495_img0.png\n",
      "   [+] Saved Diagram: p501_img0.png\n",
      "   [+] Saved Diagram: p504_img0.png\n",
      "   [+] Saved Diagram: p509_img1.png\n",
      "   [+] Saved Diagram: p517_img0.png\n",
      "   [+] Saved Diagram: p525_img0.png\n",
      "   [+] Saved Diagram: p532_img1.png\n",
      "   [+] Saved Diagram: p537_img0.png\n",
      "   [+] Saved Diagram: p542_img0.png\n",
      "   [+] Saved Diagram: p548_img1.png\n",
      "   [+] Saved Diagram: p555_img0.png\n",
      "   [+] Saved Diagram: p563_img1.png\n",
      "   [+] Saved Diagram: p571_img0.png\n",
      "   [+] Saved Diagram: p575_img1.png\n",
      "   [+] Saved Diagram: p584_img0.png\n",
      "   [+] Saved Diagram: p588_img0.png\n",
      "   [+] Saved Diagram: p620_img0.png\n",
      "   [+] Saved Diagram: p621_img0.png\n",
      "   [+] Saved Diagram: p622_img0.png\n",
      "   [+] Saved Diagram: p622_img1.png\n",
      "   [+] Saved Diagram: p623_img0.png\n",
      "   [+] Saved Diagram: p624_img0.png\n",
      "   [+] Saved Diagram: p625_img0.png\n",
      "   [+] Saved Diagram: p631_img0.png\n",
      "   [+] Saved Diagram: p631_img1.png\n",
      "   [+] Saved Diagram: p632_img0.png\n",
      "   [+] Saved Diagram: p632_img1.png\n",
      "   [+] Saved Diagram: p633_img0.png\n",
      "   [+] Saved Diagram: p633_img1.png\n",
      "   [+] Saved Diagram: p634_img0.png\n",
      "   [+] Saved Diagram: p646_img0.png\n",
      "   [+] Saved Diagram: p647_img10.png\n",
      "   [+] Saved Diagram: p661_img0.png\n",
      "   [+] Saved Diagram: p662_img10.png\n",
      "   [+] Saved Diagram: p663_img0.png\n",
      "   [+] Saved Diagram: p666_img0.png\n",
      "   [+] Saved Diagram: p691_img0.png\n",
      "   [+] Saved Diagram: p693_img0.png\n",
      "   [+] Saved Diagram: p726_img0.png\n",
      "   [+] Saved Diagram: p729_img0.png\n",
      "   [+] Saved Diagram: p732_img0.png\n",
      "   [+] Saved Diagram: p738_img0.png\n",
      "   [+] Saved Diagram: p740_img0.png\n",
      "   [+] Saved Diagram: p745_img0.png\n",
      "   [+] Saved Diagram: p810_img1.png\n",
      "   [+] Saved Diagram: p812_img0.png\n",
      "   [+] Saved Diagram: p816_img0.png\n",
      "   [+] Saved Diagram: p818_img0.png\n",
      "   [+] Saved Diagram: p820_img0.png\n",
      "   [+] Saved Diagram: p821_img0.png\n",
      "   [+] Saved Diagram: p823_img0.png\n",
      "   [+] Saved Diagram: p851_img0.png\n",
      "‚úÖ Extracted 171 visual chunks.\n",
      "üîó Added images to main chunk list. Total chunks: 2710\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 4.5: Image Extraction Pipeline (Visual Chunking) ---\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# 1. DEFINE FILE NAME (Fixes NameError)\n",
    "pdf_filename = \"sample-service-manual.pdf\" \n",
    "\n",
    "def extract_images_and_create_chunks(pdf_path, output_folder=\"extracted_images\"):\n",
    "    \"\"\"\n",
    "    1. Detects images/diagrams in the PDF.\n",
    "    2. Crops them visually (preserving labels/arrows).\n",
    "    3. Saves them to disk.\n",
    "    4. Creates a LangChain 'Document' containing the image path in metadata.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        \n",
    "    print(f\"üì∑ Scanning {pdf_path} for diagrams...\")\n",
    "    doc = fitz.open(pdf_path)\n",
    "    image_docs = []\n",
    "    \n",
    "    for page_index, page in enumerate(doc):\n",
    "        image_list = page.get_images()\n",
    "        \n",
    "        for img_index, img in enumerate(image_list):\n",
    "            xref = img[0]\n",
    "            # Get location of the image (x0, y0, x1, y1)\n",
    "            rects = page.get_image_rects(xref)\n",
    "            \n",
    "            for rect in rects:\n",
    "                # Filter small icons/logos (noise)\n",
    "                if rect.width < 150 or rect.height < 150:\n",
    "                    continue\n",
    "                \n",
    "                # Expand box slightly to catch external labels\n",
    "                clip_rect = rect + (-20, -20, 20, 20)\n",
    "                \n",
    "                # Render high-res image (3x zoom for clarity)\n",
    "                pix = page.get_pixmap(matrix=fitz.Matrix(3, 3), clip=clip_rect)\n",
    "                \n",
    "                # Save to disk\n",
    "                filename = f\"p{page_index+1}_img{img_index}.png\"\n",
    "                filepath = os.path.join(output_folder, filename)\n",
    "                pix.save(filepath)\n",
    "                \n",
    "                # Create a \"Shadow Document\" for Retrieval\n",
    "                # We put text in page_content so FAISS can find it (e.g., \"Reference Diagram\").\n",
    "                doc_text = f\"Reference Diagram: Figure on page {page_index+1}\"\n",
    "                \n",
    "                image_docs.append(Document(\n",
    "                    page_content=doc_text,\n",
    "                    metadata={\n",
    "                        \"source\": pdf_path,\n",
    "                        \"page\": page_index + 1,\n",
    "                        \"type\": \"image\",\n",
    "                        \"image_path\": filepath # <--- CRITICAL: Storing the path\n",
    "                    }\n",
    "                ))\n",
    "                print(f\"   [+] Saved Diagram: {filename}\")\n",
    "                \n",
    "    return image_docs\n",
    "\n",
    "# --- EXECUTE ---\n",
    "try:\n",
    "    image_chunks = extract_images_and_create_chunks(pdf_filename)\n",
    "    print(f\"‚úÖ Extracted {len(image_chunks)} visual chunks.\")\n",
    "    \n",
    "    # CRITICAL: Add these new chunks to your existing list!\n",
    "    # If 'chunks' exists from Cell 4, extend it. If not, create it.\n",
    "    if 'chunks' in globals():\n",
    "        chunks.extend(image_chunks)\n",
    "        print(f\"üîó Added images to main chunk list. Total chunks: {len(chunks)}\")\n",
    "    else:\n",
    "        chunks = image_chunks\n",
    "        print(\"‚ö†Ô∏è 'chunks' list not found from previous cells. Created new list.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23011a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local embedding model ...\n",
      "Creating vector store...\n",
      "Vector store created successfully using Sentence Transformers!\n"
     ]
    }
   ],
   "source": [
    "# ---CELL 5: Vector Store with Sentence Transformers ---\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS \n",
    "\n",
    "print(\"Loading local embedding model ...\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"Creating vector store...\")\n",
    "\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "print(\"Vector store created successfully using Sentence Transformers!\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "644d3927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top Retrieval Result for 'Torque specifications for suspension' ---\n",
      "2C-13 Rear Suspension:\n",
      "5)Install the swingarm. Refer to ‚ÄúSwingarm / Cushion\n",
      "Rod Removal and Installation (Page2C-8)‚Äù.\n",
      "I815H1230047-01\n",
      "Specifications\n",
      "Service Data\n",
      "B815H22307001\n",
      "Suspension\n",
      "Unit: mm (in)\n",
      "Item Standard Limit\n",
      "Rear shock absorber spring pre-set\n",
      "195 (7.7) ‚Äî\n",
      "length\n",
      "Rear shock absorber damping force Rebound 12 clicks out from stiffed position\n",
      "‚Äî\n",
      "adjuster Compression 8 clicks out from stiffed position\n",
      "Rear wheel travel 140 (5.5) ‚Äî\n",
      "Swingarm pivot shaft runout ‚Äî 0.3 (0.01)\n",
      "Tightening Torque Specifications\n",
      "B815H22307002\n",
      "Tightening torque\n",
      "Fastening part Note\n",
      "N‚ãÖm kgf-m lb-ft\n",
      "Rear shock absorber mounting nut (cid:41)(Page2C-3) /\n",
      "50 5.0 36.0\n",
      "(cid:41)(Page2C-10)\n",
      "Cushion lever mounting nut (cid:41)(Page2C-3) /\n",
      "78 7.8 56.5 (cid:41)(Page2C-6) /\n",
      "(cid:41)(Page2C-10)\n",
      "Cushion rod mounting nut (cid:41)(Page2C-6) /\n",
      "78 7.8 56.5\n",
      "(cid:41)(Page2C-10)\n",
      "Rear shock absorber lower mounting nut 50 5.0 36.0 (cid:41)(Page2C-6)\n",
      "Swingarm pivot shaft 15 1.5 11.0 (cid:41)(Page2C-9)\n",
      "Swingarm pivot nut 100 10.0 72.5 (cid:41)(Page2C-10)\n",
      "Swingarm pivot lock-nut 90 9.0 65.0 (cid:41)(Page2C-10)\n",
      "NOTE\n",
      "The specified tightening torque is also described in the following.\n",
      "‚ÄúRear Suspension Components (Page2C-1)‚Äù\n",
      "‚ÄúRear Suspension Assembly Construction (Page2C-2)‚Äù\n",
      "Reference:\n",
      "For the tightening torque of fastener not specified in this section, refer to ‚ÄúTightening Torque List in Section 0C\n",
      "(Page0C-9)‚Äù.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Test Retrieval (Debugging Step)\n",
    "test_query = \"Torque specifications for suspension\"\n",
    "results = vector_store.similarity_search(test_query, k=10)\n",
    "\n",
    "print(f\"--- Top Retrieval Result for '{test_query}' ---\")\n",
    "print(results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ba926746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 7: Define Output Structure (Updated) ---\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class VehicleSpec(BaseModel):\n",
    "    \"\"\"Information about a specific vehicle specification or procedure step.\"\"\"\n",
    "    component: str = Field(..., description=\"The part name, step number, or symptom (e.g., 'Brake Caliper', 'Step 1', 'Engine Noise').\")\n",
    "    spec_type: str = Field(..., description=\"The category (e.g., 'Torque', 'Action', 'Check').\")\n",
    "    value: str = Field(..., description=\"The primary value or instruction (e.g., '50', 'Turn Switch OFF', 'Replace Fuse').\")\n",
    "    unit: Optional[str] = Field(None, description=\"Measurement unit (Nm, PSI) if applicable.\")\n",
    "    description: Optional[str] = Field(None, description=\"Context, condition, or notes (e.g., 'Stage 1 tightening', 'If light is flashing').\")\n",
    "\n",
    "class SpecList(BaseModel):\n",
    "    \"\"\"A list of extracted vehicle specifications.\"\"\"\n",
    "    specs: List[VehicleSpec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbad88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Batch Fleet Extraction...\n",
      "   Processing: Torque specifications for front suspension...\n",
      "   ‚úÖ Found 21 items in 1.75s.\n",
      "   Processing: Fluid capacities...\n",
      "   ‚ö†Ô∏è Valid JSON, but no specific data found.\n",
      "   Processing: Emergency procedure for engine fire on ground...\n",
      "   ‚úÖ Found 5 items in 1.37s.\n",
      "   Processing: Landing gear extension speed limits...\n",
      "   ‚úÖ Found 3 items in 5.50s.\n",
      "   Processing: Troubleshooting engine starting failure...\n",
      "   ‚úÖ Found 28 items in 10.70s.\n",
      "   Processing: Chain tension adjustment...\n",
      "   ‚úÖ Found 7 items in 10.40s.\n",
      "\n",
      "üéâ DONE! Saved 64 total specs to 'vehicle_specs.json'.\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 8: Main Extraction Loop (Fleet Edition with Description) ---\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP CLOUD LLM\n",
    "# ==========================================\n",
    "GROQ_API_KEY = \"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\" #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\n",
    "\n",
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    "    api_key=GROQ_API_KEY\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 2. HELPER: Bulletproof JSON Extractor\n",
    "# ==========================================\n",
    "def extract_json_from_text(text):\n",
    "    try:\n",
    "        text = text.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "        if match: return json.loads(match.group(0))\n",
    "        return json.loads(text)\n",
    "    except: return None\n",
    "\n",
    "# ==========================================\n",
    "# 3. MASTER PROMPT (Now requests Description)\n",
    "# ==========================================\n",
    "prompt_template = \"\"\"\n",
    "You are a highly accurate technical data extractor.\n",
    "Analyze the provided text context and extract structured data for: '{question}'.\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. **COMPONENT**: The main part (e.g., \"Bolt\") or Step Number (e.g., \"Step 1\").\n",
    "2. **VALUE**: The numeric value (e.g., \"50\") or the Action (e.g., \"Turn Switch OFF\").\n",
    "3. **UNIT**: If applicable (Nm, PSI). Leave null for procedures.\n",
    "4. **DESCRIPTION**: Capture conditions (e.g., \"Initial pass\", \"If engine is hot\") or notes.\n",
    "\n",
    "Output JSON: \n",
    "{{ \"specs\": [ \n",
    "    {{ \"component\": \"...\", \"spec_type\": \"...\", \"value\": \"...\", \"unit\": \"...\", \"description\": \"...\" }} \n",
    "] }}\n",
    "\n",
    "If no relevant data is found, return: {{ \"specs\": [] }}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "# --- UPDATED FLEET QUERIES ---\n",
    "queries = [\n",
    "    # üöó CAR\n",
    "    \"Torque specifications for front suspension\",\n",
    "    \"Fluid capacities\",\n",
    "    \n",
    "    # ‚úàÔ∏è JET (Procedures)\n",
    "    \"Emergency procedure for engine fire on ground\",\n",
    "    \"Landing gear extension speed limits\",\n",
    "    \n",
    "    # üèçÔ∏è BIKE (Diagnostics)\n",
    "    \"Troubleshooting engine starting failure\",\n",
    "    \"Chain tension adjustment\"\n",
    "]\n",
    "\n",
    "all_extracted_data = []\n",
    "\n",
    "print(\"üöÄ Starting Batch Fleet Extraction...\")\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"   Processing: {query}...\")\n",
    "    start_ts = time.time()\n",
    "    \n",
    "    # Retrieve broadly\n",
    "    docs = vector_store.similarity_search(query, k=6) \n",
    "    context = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "    \n",
    "    try:\n",
    "        chain = ChatPromptTemplate.from_template(prompt_template) | llm\n",
    "        response = chain.invoke({\"context\": context, \"question\": query})\n",
    "        \n",
    "        data = extract_json_from_text(response.content)\n",
    "        \n",
    "        if data and \"specs\" in data:\n",
    "            items = data[\"specs\"]\n",
    "            if items:\n",
    "                all_extracted_data.extend(items)\n",
    "                print(f\"   ‚úÖ Found {len(items)} items in {time.time()-start_ts:.2f}s.\")\n",
    "            else:\n",
    "                print(\"   ‚ö†Ô∏è Valid JSON, but no specific data found.\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è No JSON found.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    time.sleep(3)\n",
    "\n",
    "# Save\n",
    "with open(\"vehicle_specs.json\", \"w\") as f:\n",
    "    json.dump(all_extracted_data, f, indent=4)\n",
    "\n",
    "print(f\"\\nüéâ DONE! Saved {len(all_extracted_data)} total specs to 'vehicle_specs.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2969ad20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to vehicle_specs.json\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Save and View Results\n",
    "\n",
    "import json\n",
    "\n",
    "# Save to JSON file\n",
    "output_file = \"vehicle_specs.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(all_extracted_data, f, indent=4)\n",
    "\n",
    "print(f\"Saved data to {output_file}\")\n",
    "\n",
    "# Display first 5 results\n",
    "print(json.dumps(all_extracted_data[:5], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d190478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Index saved to folder 'faiss_db_index_test'\n"
     ]
    }
   ],
   "source": [
    "# Run this in your notebook to save the index to disk\n",
    "vector_store.save_local(\"faiss_db_index_test\")\n",
    "print(\"‚úÖ Index saved to folder 'faiss_db_index_test'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "41d9105c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector Store is active and ready.\n"
     ]
    }
   ],
   "source": [
    "# --- SAFETY CHECK: Ensure Vector Store Exists ---\n",
    "import os\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# 1. Define Embeddings (Required to load the DB)\n",
    "if 'embeddings' not in globals():\n",
    "    print(\"üîÑ Initializing embeddings model...\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. Check if vector_store is active. If not, try to load it.\n",
    "if 'vector_store' not in globals():\n",
    "    print(\"‚ö†Ô∏è 'vector_store' variable not found in memory.\")\n",
    "    \n",
    "    # Check if a saved index exists on disk\n",
    "    # NOTE: Make sure this folder name matches what you saved in Cell 11 (\"faiss_db_index_test\")\n",
    "    index_folder = \"faiss_db_index_test\" \n",
    "    \n",
    "    if os.path.exists(index_folder):\n",
    "        print(f\"üìÇ Found saved index in '{index_folder}'. Loading...\")\n",
    "        vector_store = FAISS.load_local(index_folder, embeddings, allow_dangerous_deserialization=True)\n",
    "        print(\"‚úÖ Vector Store loaded successfully!\")\n",
    "    else:\n",
    "        print(\"‚ùå CRITICAL ERROR: No 'vector_store' found in memory OR on disk.\")\n",
    "        print(\"üëâ PLEASE RUN CELL 5 & 6 to create the database first!\")\n",
    "        raise Exception(\"Database missing. Run the 'Create Vector Store' cell first.\")\n",
    "else:\n",
    "    print(\"‚úÖ Vector Store is active and ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "33a90876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "%pip install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b1664a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Inspecting Database Content...\n",
      "‚úÖ SUCCESS: Found Jet data in the database!\n",
      "   Sample: T.O. GR1F(cid:6)16CJ(cid:6)1\n",
      "Hybrid (HYB) Engine HYD/OIL PRESS Warning\n",
      "Operation GE129 . . . . . . ....\n"
     ]
    }
   ],
   "source": [
    "# Check if \"jet\" data exists in the vector store\n",
    "print(\"üîç Inspecting Database Content...\")\n",
    "try:\n",
    "    # Search for a generic term with the Jet filter\n",
    "    test_docs = vector_store.similarity_search(\"engine\", k=1, filter={\"vehicle_type\": \"jet\"})\n",
    "    if test_docs:\n",
    "        print(\"‚úÖ SUCCESS: Found Jet data in the database!\")\n",
    "        print(f\"   Sample: {test_docs[0].page_content[:100]}...\")\n",
    "    else:\n",
    "        print(\"‚ùå FAILURE: No Jet data found. Did you run Cell 5?\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2a30c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è STARTING SYSTEM REBUILD...\n",
      "   üìñ Reading CAR manual...\n",
      "      -> Extracted 852 text chunks.\n",
      "   üìñ Reading JET manual...\n",
      "      -> Extracted 513 text chunks.\n",
      "   üìñ Reading BIKE manual...\n",
      "      -> Extracted 659 text chunks.\n",
      "   üì∑ Scanning for Diagrams...\n",
      "      -> Saved 171 diagrams from car.\n",
      "      -> Saved 2 diagrams from jet.\n",
      "      -> Saved 103 diagrams from bike.\n",
      "   üß† Building Vector Brain (This may take a moment)...\n",
      "\n",
      "‚úÖ SYSTEM READY! Total Knowledge Base: 2300 records.\n",
      "üëâ You can now run Cell 13 (The UI).\n"
     ]
    }
   ],
   "source": [
    "# --- MASTER CELL: REBUILD FLEET DATABASE (Text + Images) ---\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "import time\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION (Define the Fleet)\n",
    "# ==========================================\n",
    "FLEET_CONFIG = {\n",
    "    \"car\": \"sample-service-manual.pdf\",       \n",
    "    \"jet\": \"HAF-F16.pdf\",                     \n",
    "    \"bike\": \"motercycles.pdf\"                 \n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è STARTING SYSTEM REBUILD...\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. TEXT & TABLE EXTRACTION (The \"Reader\")\n",
    "# ==========================================\n",
    "all_chunks = []\n",
    "\n",
    "def extract_text_smart(pdf_path, v_type):\n",
    "    local_chunks = []\n",
    "    if not os.path.exists(pdf_path): return []\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            # Try Table\n",
    "            table = page.extract_table()\n",
    "            text = page.extract_text()\n",
    "            \n",
    "            content = \"\"\n",
    "            if table:\n",
    "                # Flatten table row by row\n",
    "                for row in table[:10]: # Limit to first 10 rows to save space\n",
    "                    clean_row = [str(cell).replace('\\n', ' ') for cell in row if cell]\n",
    "                    content += \" | \".join(clean_row) + \"\\n\"\n",
    "                local_chunks.append(Document(page_content=content, metadata={\"source\": pdf_path, \"page\": i+1, \"vehicle_type\": v_type}))\n",
    "            elif text:\n",
    "                local_chunks.append(Document(page_content=text, metadata={\"source\": pdf_path, \"page\": i+1, \"vehicle_type\": v_type}))\n",
    "                \n",
    "    return local_chunks\n",
    "\n",
    "for v_type, path in FLEET_CONFIG.items():\n",
    "    print(f\"   üìñ Reading {v_type.upper()} manual...\")\n",
    "    chunks = extract_text_smart(path, v_type)\n",
    "    all_chunks.extend(chunks)\n",
    "    print(f\"      -> Extracted {len(chunks)} text chunks.\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. IMAGE EXTRACTION (The \"Photographer\")\n",
    "# ==========================================\n",
    "output_folder = \"extracted_images\"\n",
    "if not os.path.exists(output_folder): os.makedirs(output_folder)\n",
    "\n",
    "def extract_images_smart(pdf_path, v_type):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    img_docs = []\n",
    "    \n",
    "    for i, page in enumerate(doc):\n",
    "        image_list = page.get_images()\n",
    "        for img_idx, img in enumerate(image_list):\n",
    "            xref = img[0]\n",
    "            rects = page.get_image_rects(xref)\n",
    "            \n",
    "            for rect in rects:\n",
    "                if rect.width < 150 or rect.height < 150: continue # Skip small icons\n",
    "                \n",
    "                # Context Expansion (Padding)\n",
    "                clip = rect + (-20, -20, 20, 20)\n",
    "                pix = page.get_pixmap(matrix=fitz.Matrix(3, 3), clip=clip)\n",
    "                \n",
    "                filename = f\"{v_type}_p{i+1}_{img_idx}.png\"\n",
    "                filepath = os.path.join(output_folder, filename)\n",
    "                pix.save(filepath)\n",
    "                \n",
    "                # SHADOW DOCUMENT (This links the Image to the Brain)\n",
    "                # We add keywords like \"Diagram\", \"Figure\", \"Schematic\" to help the search find it.\n",
    "                desc = f\"Reference Diagram Figure Schematic for {v_type} on page {i+1}.\"\n",
    "                img_docs.append(Document(\n",
    "                    page_content=desc, \n",
    "                    metadata={\"source\": pdf_path, \"page\": i+1, \"vehicle_type\": v_type, \"image_path\": filepath, \"type\": \"image\"}\n",
    "                ))\n",
    "    return img_docs\n",
    "\n",
    "print(\"   üì∑ Scanning for Diagrams...\")\n",
    "for v_type, path in FLEET_CONFIG.items():\n",
    "    if os.path.exists(path):\n",
    "        imgs = extract_images_smart(path, v_type)\n",
    "        all_chunks.extend(imgs)\n",
    "        print(f\"      -> Saved {len(imgs)} diagrams from {v_type}.\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. EMBEDDING (Creating the Brain)\n",
    "# ==========================================\n",
    "print(\"   üß† Building Vector Brain (This may take a moment)...\")\n",
    "\n",
    "# Define Embedding Model (HuggingFace)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create Vector Store\n",
    "vector_store = FAISS.from_documents(all_chunks, embedding_model)\n",
    "\n",
    "print(f\"\\n‚úÖ SYSTEM READY! Total Knowledge Base: {len(all_chunks)} records.\")\n",
    "print(\"üëâ You can now run Cell 13 (The UI).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f22e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1421009/3024078071.py:189: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  txt_input.on_submit(on_submit)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8eb9c7751ce4c2eafea08644ec840aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"\\n<style>\\n    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- CELL 13: Mechanic AI \"Pro\" Interface (Final + Images) ---\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, Image, clear_output\n",
    "import json\n",
    "import re\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# ============================================\n",
    "# 1. CONFIGURATION\n",
    "# ============================================\n",
    "# üëá PASTE YOUR ACTUAL KEY HERE\n",
    "GROQ_API_KEY = \"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\" \n",
    "\n",
    "# Brain\n",
    "gui_llm = ChatGroq(temperature=0.1, model_name=\"llama-3.1-8b-instant\", api_key=GROQ_API_KEY)\n",
    "\n",
    "# ============================================\n",
    "# 2. PROMPTS & LOGIC\n",
    "# ============================================\n",
    "rag_prompt = \"\"\"\n",
    "You are a technical data extractor. Analyze Context for: '{question}'.\n",
    "RULES:\n",
    "1. Return ONLY valid JSON.\n",
    "2. Structure: {{ \"specs\": [ {{ \"component\": \"...\", \"value\": \"...\", \"unit\": \"...\", \"description\": \"...\" }} ] }}\n",
    "3. If NOT found, return {{ \"specs\": [] }}\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "general_prompt = \"\"\"\n",
    "The user asked: '{question}'.\n",
    "We searched the manuals but found NO specific match.\n",
    "Answer based on general mechanical knowledge. Be concise.\n",
    "\"\"\"\n",
    "\n",
    "def clean_json(text):\n",
    "    try:\n",
    "        text = str(text).replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "        if match: return json.loads(match.group(0))\n",
    "    except: pass\n",
    "    return {\"specs\": []}\n",
    "\n",
    "def detect_filter(query):\n",
    "    q = query.lower()\n",
    "    if any(x in q for x in [\"f-16\", \"jet\", \"aircraft\"]): return {\"vehicle_type\": \"jet\"}\n",
    "    if any(x in q for x in [\"bike\", \"motorcycle\", \"ducati\"]): return {\"vehicle_type\": \"bike\"}\n",
    "    if any(x in q for x in [\"car\", \"ford\", \"f-150\"]): return {\"vehicle_type\": \"car\"}\n",
    "    return None\n",
    "\n",
    "def handle_search(query):\n",
    "    # 1. CHIT-CHAT\n",
    "    if query.lower().strip() in [\"hi\", \"hello\", \"help\"]:\n",
    "        return {\"type\": \"chat\", \"content\": \"<b>System Ready.</b><br>I have access to F-16, Ducati, and Ford F-150 manuals.<br>Select a query below or type your own.\"}\n",
    "\n",
    "    # 2. SEARCH\n",
    "    active_filter = detect_filter(query)\n",
    "    try:\n",
    "        if active_filter: docs = vector_store.similarity_search(query, k=4, filter=active_filter)\n",
    "        else: docs = vector_store.similarity_search(query, k=4)\n",
    "    except: docs = []\n",
    "\n",
    "    # 3. EXTRACT TEXT & IMAGES\n",
    "    if docs:\n",
    "        # A. Extract Text Context\n",
    "        context = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "        \n",
    "        # B. Extract Images (Deduplicate them)\n",
    "        found_images = []\n",
    "        seen_paths = set()\n",
    "        for d in docs:\n",
    "            path = d.metadata.get(\"image_path\")\n",
    "            if path and path not in seen_paths:\n",
    "                found_images.append(path)\n",
    "                seen_paths.add(path)\n",
    "\n",
    "        # C. Generate Answer\n",
    "        chain = ChatPromptTemplate.from_template(rag_prompt) | gui_llm\n",
    "        response = chain.invoke({\"context\": context, \"question\": query})\n",
    "        data = clean_json(response.content)\n",
    "        \n",
    "        if data.get(\"specs\"):\n",
    "            return {\n",
    "                \"type\": \"manual\", \n",
    "                \"specs\": data[\"specs\"], \n",
    "                \"images\": found_images, # <--- Passing images to UI\n",
    "                \"source\": active_filter['vehicle_type'].upper() if active_filter else \"DOCS\"\n",
    "            }\n",
    "            \n",
    "    # 4. FALLBACK\n",
    "    gen_chain = ChatPromptTemplate.from_template(general_prompt) | gui_llm\n",
    "    gen_response = gen_chain.invoke({\"question\": query})\n",
    "    return {\"type\": \"general\", \"content\": gen_response.content}\n",
    "\n",
    "# ============================================\n",
    "# 3. UI STYLING\n",
    "# ============================================\n",
    "style = \"\"\"\n",
    "<style>\n",
    "    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap');\n",
    "    .app-container { font-family: 'Inter', sans-serif; max-width: 900px; margin: 0 auto; color: #333; }\n",
    "    .header { padding: 20px 0; border-bottom: 1px solid #eee; margin-bottom: 20px; }\n",
    "    .header h1 { font-size: 22px; font-weight: 600; margin: 0; color: #111; display: flex; align-items: center; gap: 10px; }\n",
    "    .header-sub { color: #666; font-size: 14px; margin-top: 5px; }\n",
    "    .response-card { background: #fff; border: 1px solid #e0e0e0; border-radius: 12px; padding: 24px; box-shadow: 0 4px 12px rgba(0,0,0,0.05); animation: fadeIn 0.4s ease; }\n",
    "    .data-table { width: 100%; border-collapse: collapse; margin-top: 15px; font-size: 14px; }\n",
    "    .data-table th { text-align: left; color: #888; font-weight: 500; padding: 8px 0; border-bottom: 1px solid #eee; }\n",
    "    .data-table td { padding: 12px 0; border-bottom: 1px solid #f5f5f5; vertical-align: top; }\n",
    "    .val-text { font-weight: 600; color: #222; }\n",
    "    .desc-text { color: #666; font-style: italic; }\n",
    "    .tag { display: inline-block; padding: 4px 8px; border-radius: 6px; font-size: 11px; font-weight: 600; letter-spacing: 0.5px; }\n",
    "    .tag-manual { background: #d3f9d8; color: #2b8a3e; }\n",
    "    .tag-ai { background: #fff3bf; color: #f08c00; }\n",
    "    @keyframes fadeIn { from { opacity: 0; transform: translateY(10px); } to { opacity: 1; transform: translateY(0); } }\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "# ============================================\n",
    "# 4. WIDGET CONSTRUCTION\n",
    "# ============================================\n",
    "header_html = widgets.HTML(f\"{style}<div class='app-container'><div class='header'><h1>‚ö° Mechanic AI <span style='font-size:12px; background:#eee; padding:2px 6px; border-radius:4px; color:#555;'>FLEET COMMAND</span></h1><div class='header-sub'>Multi-Modal Retrieval System (v3.0 Final)</div></div>\")\n",
    "\n",
    "# Chips\n",
    "btn_layout = widgets.Layout(width='98%', margin='2px')\n",
    "btn_car1 = widgets.Button(description=\"Suspension Torque (Car)\", icon='car', layout=btn_layout)\n",
    "btn_car1.tooltip = \"Torque specifications for front suspension (Car)\"\n",
    "btn_car2 = widgets.Button(description=\"Fluid Capacities (Car)\", icon='tint', layout=btn_layout)\n",
    "btn_car2.tooltip = \"Fluid capacities (Car)\"\n",
    "btn_jet1 = widgets.Button(description=\"Engine Fire Proc. (Jet)\", icon='plane', layout=btn_layout)\n",
    "btn_jet1.tooltip = \"Emergency procedure for engine fire on ground (F-16)\"\n",
    "btn_jet2 = widgets.Button(description=\"Gear Speed Limits (Jet)\", icon='tachometer', layout=btn_layout)\n",
    "btn_jet2.tooltip = \"Landing gear extension speed limits (F-16)\"\n",
    "btn_bike1 = widgets.Button(description=\"Start Failure (Bike)\", icon='motorcycle', layout=btn_layout)\n",
    "btn_bike1.tooltip = \"Troubleshooting engine starting failure (Bike)\"\n",
    "btn_bike2 = widgets.Button(description=\"Chain Tension (Bike)\", icon='cogs', layout=btn_layout)\n",
    "btn_bike2.tooltip = \"Chain tension adjustment (Bike)\"\n",
    "\n",
    "col_car = widgets.VBox([widgets.HTML(\"<b>üöó Ford F-150</b>\"), btn_car1, btn_car2], layout=widgets.Layout(width='33%'))\n",
    "col_jet = widgets.VBox([widgets.HTML(\"<b>‚úàÔ∏è F-16 Jet</b>\"), btn_jet1, btn_jet2], layout=widgets.Layout(width='33%'))\n",
    "col_bike = widgets.VBox([widgets.HTML(\"<b>üèçÔ∏è Ducati</b>\"), btn_bike1, btn_bike2], layout=widgets.Layout(width='33%'))\n",
    "chip_container = widgets.HBox([col_car, col_jet, col_bike], layout=widgets.Layout(width='100%', margin='0 0 20px 0'))\n",
    "\n",
    "# Input\n",
    "txt_input = widgets.Text(placeholder=\"Ask a question...\", layout=widgets.Layout(width='85%'))\n",
    "btn_send = widgets.Button(icon='paper-plane', layout=widgets.Layout(width='10%'))\n",
    "input_area = widgets.HBox([txt_input, btn_send])\n",
    "out_display = widgets.Output()\n",
    "\n",
    "def on_submit(b):\n",
    "    if isinstance(b, widgets.Button) and hasattr(b, 'tooltip') and b.tooltip:\n",
    "        query = b.tooltip\n",
    "        txt_input.value = query \n",
    "    else:\n",
    "        query = txt_input.value\n",
    "        \n",
    "    if not query: return\n",
    "    out_display.clear_output()\n",
    "    with out_display:\n",
    "        display(HTML(f\"<div style='color:#666; margin-top:20px;'>‚å¨ Processing <b>'{query}'</b>...</div>\"))\n",
    "        try:\n",
    "            res = handle_search(query)\n",
    "            out_display.clear_output()\n",
    "            \n",
    "            if res['type'] == 'manual':\n",
    "                html = f\"<div class='response-card'><div><span class='tag tag-manual'>‚úì MANUAL SOURCE: {res['source']}</span></div>\"\n",
    "                rows = \"\".join([f\"<tr><td>{x['component']}</td><td class='val-text'>{x['value']} {x.get('unit','') or ''}</td><td class='desc-text'>{x.get('description','-')}</td></tr>\" for x in res['specs']])\n",
    "                html += f\"<table class='data-table'><thead><tr><th width='30%'>Component / Step</th><th width='25%'>Value / Action</th><th>Notes</th></tr></thead><tbody>{rows}</tbody></table>\"\n",
    "                display(HTML(html + \"</div>\"))\n",
    "                \n",
    "                # --- IMAGE DISPLAY LOGIC ---\n",
    "                if res.get('images') and len(res['images']) > 0:\n",
    "                    display(HTML(\"<div style='margin-top:20px; font-weight:600; color:#444; border-top:1px solid #eee; padding-top:15px;'>üì∑ Visual Reference:</div>\"))\n",
    "                    for img_path in res['images']:\n",
    "                        try:\n",
    "                            display(Image(filename=img_path, width=600))\n",
    "                            display(HTML(f\"<div style='color:#888; font-size:11px; margin-bottom:15px;'>Source: {img_path}</div>\"))\n",
    "                        except:\n",
    "                            display(HTML(f\"<div style='color:red;'>‚ö†Ô∏è Image missing on disk: {img_path}</div>\"))\n",
    "                            \n",
    "            elif res['type'] == 'general':\n",
    "                display(HTML(f\"<div class='response-card'><span class='tag tag-ai'>‚ö† GENERAL KNOWLEDGE</span><div style='margin-top:10px; color:#444; line-height:1.6;'>{res['content']}</div><div style='margin-top:15px; font-size:12px; color:#888; border-top:1px solid #eee; padding-top:10px;'>*Data not found in official fleet documents. Response generated by AI logic.</div></div>\"))\n",
    "            elif res['type'] == 'chat':\n",
    "                display(HTML(f\"<div class='response-card' style='background:#f8f9fa;'>{res['content']}</div>\"))\n",
    "        except Exception as e:\n",
    "             display(HTML(f\"<div style='color:red;'>‚ùå Error: {e}</div>\"))\n",
    "\n",
    "btn_send.on_click(on_submit)\n",
    "txt_input.on_submit(on_submit)\n",
    "for btn in [btn_car1, btn_car2, btn_jet1, btn_jet2, btn_bike1, btn_bike2]: btn.on_click(on_submit)\n",
    "\n",
    "display(widgets.VBox([header_html, chip_container, input_area, out_display], layout=widgets.Layout(max_width='900px')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148f0fda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
